# Base training configuration

model:
  latent_dim: 256
  action_dim: 4
  hidden_dim: 512
  num_layers: 4

data:
  sequence_length: 10
  image_size: [256, 256]
  
training:
  batch_size: 16
  num_epochs: 100
  use_amp: true
  grad_accum_steps: 1
  grad_clip: 1.0
  
  log_interval: 100
  val_interval: 1000
  checkpoint_dir: "./checkpoints"

optimizer:
  type: "adamw"
  lr: 1.0e-4
  weight_decay: 0.01
  scheduler: "cosine"

loss:
  recon_weight: 1.0
  pred_weight: 1.0
  perceptual_weight: 0.1
  latent_reg_weight: 0.01